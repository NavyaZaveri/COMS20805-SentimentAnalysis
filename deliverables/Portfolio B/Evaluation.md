# Evaluation

We determined the usefulness of our application through evaluating its utility and usability. We have conducted a qualitative and quantitative study through an online questionnaire and task based evaluation, by observing a participant trying out the system and measuring certain variables. We believe that using an online questionnaire was the right approach as a user talk-through would not give the participant enough time to reflect and it interferes with the interactive experience. Moreover, we felt that conducting an interview would not make the participants feel conscious in what they are doing. This investigation was conducted with the help of 15 Computer Science and Engineering students from the University of Bristol. The quantitative study was conducted in the Merchant Venturers Building and insights from the study were recorded by taking notes.

## Qualitative Analysis
This research was conducted using an online questionnaire the participants could fill in once they have used the application. Our results showed that 16.7% of participants found the results to be vague. This was mainly because no context was shown, as to how the results were calculated and what the scores represent. We have rectified this by providing more detail. Furthermore, 29% of the users were not familiar with the term Open-Source Software. This motivated usto  provide a short description as to what our application aims to achieve and what queries are expected. In addition, participants found that when hovering over scores in the graph, data displayed over each point overlapped with one another. This made it hard for users to interpret the graph. This was fixed by removing the year for each data point. Finally, 14% users suggested to make the site more aesthetic.

## Quantitative Analysis
We conducted this study by having a task based interview with the participants, asking them to search for a query and tell us what the results suggest about the query. On the first attempt there were 4 successful and 3 unsuccessful tries from the participants. We found that when users were not provided any context as to what to search for, they would search for irrelevant queries leading to a failed entry. However if information was provided, there were no failed attempts. Given an appropriate query, it took around 1 to 2 seconds for the results to be displayed. Partakers were satisfied with this. In addition, users who provided a suitable query took on average about 5 seconds to interpret the results displayed. In conclusion, we deduced that users interpret the results well and understand the main purpose of the site provided some context is provided.

## Conclusion
Based on our evaluation study, our final changes to our development stage were as follows:

### Stage 1
1. Search Service provided.
2. List of posts for a given query can be displayed following its sentiment score, post URL and the date the post was released.

### Stage 2
1. Historical plot of post can be displayed.
2. Spam Classifier included.
3. More posts can be displayed.

### Final Changes
1. Colour coded scores for better understanding.
2. More context provided as to what the application is mainly used for and what the results suggest.
3. Visually more pleasing.
